{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "data_dir = './data/Seinfeld_Scripts.txt'\n",
    "text = helper.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 46367\n",
      "Number of lines: 109233\n",
      "Average number of words in each line: 5.544240293684143\n",
      "\n",
      "The lines 0 to 10:\n",
      "jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. \n",
      "\n",
      "jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother. \n",
      "\n",
      "george: are you through? \n",
      "\n",
      "jerry: you do of course try on, when you buy? \n",
      "\n",
      "george: yes, it was purple, i liked it, i dont actually recall considering the buttons. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_line_range = (0, 10)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    word_counts = Counter(text)\n",
    "    \n",
    "    # sorting the words from most to least frequent in text occurrence\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    \n",
    "    # create int_to_vocab dictionaries\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    tokens = dict()\n",
    "    tokens['.'] = '<PERIOD>'\n",
    "    tokens[','] = '<COMMA>'\n",
    "    tokens['\"'] = '<QUOTATION_MARK>'\n",
    "    tokens[';'] = '<SEMICOLON>'\n",
    "    tokens['!'] = '<EXCLAMATION_MARK>'\n",
    "    tokens['?'] = '<QUESTION_MARK>'\n",
    "    tokens['('] = '<LEFT_PAREN>'\n",
    "    tokens[')'] = '<RIGHT_PAREN>'\n",
    "    tokens['?'] = '<QUESTION_MARK>'\n",
    "    tokens['-'] = '<DASH>'\n",
    "    tokens['\\n'] = '<NEW_LINE>'\n",
    "    return tokens \n",
    "        \n",
    "\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "\n",
    "    n_batches = len(words)//batch_size\n",
    "\n",
    "    words = words[:n_batches*batch_size]\n",
    "    y_len = len(words) - sequence_length\n",
    "    x, y = [], []\n",
    "    for idx in range(0, y_len):\n",
    "        idx_end = sequence_length + idx\n",
    "        x_batch = words[idx:idx_end]\n",
    "        x.append(x_batch)\n",
    "\n",
    "        batch_y =  words[idx_end]\n",
    "\n",
    "        y.append(batch_y)    \n",
    "\n",
    "\n",
    "    data = TensorDataset(torch.from_numpy(np.asarray(x)), torch.from_numpy(np.asarray(y)))\n",
    "\n",
    "    data_loader = DataLoader(data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    return data_loader    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[  0,   1,   2,   3,   4],\n",
      "        [  1,   2,   3,   4,   5],\n",
      "        [  2,   3,   4,   5,   6],\n",
      "        [  3,   4,   5,   6,   7],\n",
      "        [  4,   5,   6,   7,   8],\n",
      "        [  5,   6,   7,   8,   9],\n",
      "        [  6,   7,   8,   9,  10],\n",
      "        [  7,   8,   9,  10,  11],\n",
      "        [  8,   9,  10,  11,  12],\n",
      "        [  9,  10,  11,  12,  13]])\n",
      "torch.Size([10])\n",
      "tensor([  5,   6,   7,   8,   9,  10,  11,  12,  13,  14])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5,lr=0.001):\n",
    "\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "    \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, nn_input, hidden):\n",
    "\n",
    "\n",
    "        batch_size = nn_input.size(0)\n",
    "        embeds = self.embedding(nn_input)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.fc(lstm_out)\n",
    "        \n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "        out = out[:, -1]\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "tests.test_rnn(RNN, train_on_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        rnn.cuda()\n",
    "\n",
    "    h = tuple([each.data for each in hidden])\n",
    "\n",
    "\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        inputs, target = inp.cuda(), target.cuda()\n",
    "\n",
    "    output, h = rnn(inputs, h)\n",
    "    \n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
    "\n",
    "    optimizer.step()\n",
    "    return loss.item(), h\n",
    "\n",
    "tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sequence_length =  10\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21388\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 5\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "vocab_size = len(vocab_to_int)\n",
    "\n",
    "output_size = vocab_size\n",
    "\n",
    "embedding_dim = 200\n",
    "\n",
    "hidden_dim = 250\n",
    "\n",
    "n_layers = 2\n",
    "\n",
    "show_every_n_batches = 2000\n",
    "\n",
    "print(len(vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5 epoch(s)...\n",
      "Epoch:    1/5     Loss: 4.947368633985519\n",
      "\n",
      "Epoch:    1/5     Loss: 4.498962902665138\n",
      "\n",
      "Epoch:    1/5     Loss: 4.355078360199928\n",
      "\n",
      "Epoch:    2/5     Loss: 4.116275652961911\n",
      "\n",
      "Epoch:    2/5     Loss: 3.948169407486916\n",
      "\n",
      "Epoch:    2/5     Loss: 3.910545413374901\n",
      "\n",
      "Epoch:    3/5     Loss: 3.806903993424701\n",
      "\n",
      "Epoch:    3/5     Loss: 3.7283408836126326\n",
      "\n",
      "Epoch:    3/5     Loss: 3.721267277598381\n",
      "\n",
      "Epoch:    4/5     Loss: 3.642319423088809\n",
      "\n",
      "Epoch:    4/5     Loss: 3.5875446372032167\n",
      "\n",
      "Epoch:    4/5     Loss: 3.588407546877861\n",
      "\n",
      "Epoch:    5/5     Loss: 3.530836515751168\n",
      "\n",
      "Epoch:    5/5     Loss: 3.487316879153252\n",
      "\n",
      "Epoch:    5/5     Loss: 3.4958903646469115\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "\n",
    "helper.save_model('./save/trained_rnn', trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How did you decide on your model hyperparameters? \n",
    "For example, did you try different sequence_lengths and find that one size made the model converge faster? What about your hidden_dim and n_layers; how did you decide on those?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "\n",
    "\n",
    "I tried:\n",
    "- sequence_length =  10, batch_size = 64, learning_rate = 0.01, embedding_dim = 200, hidden_dim = 200, n_layers = 2. Started with loss 9.25 and after 4 epochs the loss was still around 9.26.\n",
    "- sequence_length =  10, batch_size = 64, learning_rate = 0.003, embedding_dim = 300, hidden_dim = 250, n_layers = 2 Started with Loss: 9.202159190654754 and at epoch 4 it was Loss: 9.206429640371343\n",
    "- sequence_length =  20, batch_size = 20, learning_rate = 0.3, embedding_dim = 300, hidden_dim = 250, n_layers = 2 Started with Loss: 9.70091618013382, and at epoch 4 it was still around 9.6\n",
    "- sequence_length =  20, batch_size = 124, learning_rate = 1, embedding_dim = 200, hidden_dim = 200, n_layers = 2 Started with Epoch: 1/10 Loss: 9.50547212076187 .. \n",
    "\n",
    "At this point i had some bugs in my code related to zero_grad, extra dropout layer and sigmoid layer.\n",
    "Fixed issues and retried:\n",
    "\n",
    "- sequence_length =  10, batch_size = 128, learning_rate = 0.001, embedding_dim = 200, hidden_dim = 250, n_layers = 2 Started with:\n",
    "    Training for 10 epoch(s)...\n",
    "    Epoch:    1/10    Loss: 4.944083527803421\n",
    "    ...\n",
    "    Epoch:    4/10    Loss: 3.5780555000305174\n",
    "    ...\n",
    "    Epoch:    7/10    Loss: 3.3266124720573425\n",
    "    ...\n",
    "    \n",
    "- sequence_length =  10, batch_size = 124, learning_rate = 0.1, embedding_dim = 200, hidden_dim = 200, n_layers = 2 Started with \n",
    "    Training for 10 epoch(s)... \n",
    "    Epoch:    1/5    Loss: 5.481069218158722\n",
    "    Epoch:    2/5    Loss: 5.025624033570289\n",
    "    Epoch:    3/5   Loss: 4.981013494968415\n",
    "    \n",
    "I stopped here, because, even if it was decreasing it seemd to converge way slower than the previous experiment with a lower learning rate and a slightly bigger hidden_dim.\n",
    "\n",
    "The first experiment above reached:\n",
    "`Epoch:   5/5    Loss: 3.4958903646469115`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "trained_rnn = helper.load_model('./save/trained_rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "\n",
    "    rnn.eval()\n",
    "    \n",
    "\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() \n",
    "         \n",
    "\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jerry: rise.\n",
      "\n",
      "hoyt: the honor.\n",
      "\n",
      "jerry: you know what?\n",
      "\n",
      "kramer: yeah, i can't believe that was the first thing.\n",
      "\n",
      "[new witness: the phone\n",
      "\n",
      "chiles: what do you want to do?\n",
      "\n",
      "hoyt: what do you want to do?\n",
      "\n",
      "elaine: no.\n",
      "\n",
      "hoyt: what about the hell?\n",
      "\n",
      "hoyt: you know what you want to do with the defendants?\n",
      "\n",
      "elaine: what?!\n",
      "\n",
      "elaine: no, i don't know.\n",
      "\n",
      "hoyt: what?\n",
      "\n",
      "george: yeah.\n",
      "\n",
      "elaine: yeah! i think you are in the bottom of my shirt.\n",
      "\n",
      "hoyt: you can't believe the defendants is going to be a little bystander.\n",
      "\n",
      "hoyt: i think it's the most time.\n",
      "\n",
      "elaine: oh.\n",
      "\n",
      "hoyt: i don't want a little bystander to get a little bystander.\n",
      "\n",
      "george: what?\n",
      "\n",
      "hoyt: no, that's not the most exciting.\n",
      "\n",
      "hoyt: you know, i think i could have watched it.\n",
      "\n",
      "hoyt: the defendants- bone!\n",
      "\n",
      "jerry: i can't do to see this. it's like a little mishap.\n",
      "\n",
      "[new witness: voice:\n",
      "\n",
      "george: so what is this?\n",
      "\n",
      "hoyt: what do you think?\n",
      "\n",
      "jerry: oh, yeah.\n",
      "\n",
      "hoyt: what about that?\n",
      "\n",
      "jerry: i don't know what i think that i got it, but i have a lot of selfishness. i mean, i can't believe that you were.\n",
      "\n",
      "hoyt: the bubble guard is in a plane, you know.\n",
      "\n",
      "[new witness: voice:\n",
      "\n",
      "[new witness: voice:\n",
      "\n",
      "hoyt: what?\n",
      "\n",
      "jerry: yeah, yeah.\n",
      "\n",
      "george: i was going to the whole fling to the time, but i don't know what i can be in the time.\n",
      "\n",
      "hoyt: no, i don't know.\n",
      "\n",
      "hoyt: you know, what happened to you?\n",
      "\n",
      "elaine: i can't get a little adjustment.\n",
      "\n",
      "[new witness: marcellino ross\n",
      "\n",
      "jerry: no.\n",
      "\n",
      "hoyt: i can't believe that\n"
     ]
    }
   ],
   "source": [
    "# run the cell multiple times to get different results!\n",
    "gen_length = 400 # modify the length to your preference\n",
    "prime_word = 'jerry' # name for starting the script\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f =  open(\"generated_script_2.txt\",\"w\")\n",
    "f.write(generated_script)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
